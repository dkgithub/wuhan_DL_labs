{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with Keras 2\n",
    "\n",
    "### 5th Summer School on INtelligent signal processing for FrontIEr Research and Industry, Wuhan, May 2019\n",
    "\n",
    "will be at dkgithub\n",
    "\n",
    "### GPU Usage\n",
    "\n",
    "Before we start, let us check if we have a GPU available. Deep learning needs efficient numerical linear algebra calculations. https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms\n",
    "They are provided by special libraries (openBLAS, MKL(Intel), cuBLAS(NVidia)).\n",
    "Graphical processing units (GPU) can be used as accelerators for vector and matrix operations.\n",
    "\n",
    "Keras/Tensorflow will automatically use an available GPU. Provided that the necessary CUDA libraries from NVidia are available and that the GPU version of Tensorflow is available.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the `device_type` and some kind of 'GPU'.\n",
    "***\n",
    "Next, we define a function to plot learning curves. We will need this later. Just click the cell and continue to the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "def plotLearningCurves(*histObjs):\n",
    "    \"\"\"This function processes all histories given in the tuple.\n",
    "    Left losses, right accuracies\n",
    "    \"\"\"\n",
    "    # too many plots\n",
    "    if len(histObjs)>10: \n",
    "        print('Too many objects!')\n",
    "        return\n",
    "    # missing names\n",
    "    for histObj in histObjs:\n",
    "        if not hasattr(histObj, 'name'): histObj.name='?'\n",
    "    names=[]\n",
    "    # loss plot\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    # loop through arguments\n",
    "    for histObj in histObjs:\n",
    "        plt.plot(histObj.history['loss'])\n",
    "        names.append('train '+histObj.name)\n",
    "        plt.plot(histObj.history['val_loss'])\n",
    "        names.append('validation '+histObj.name)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(names, loc='upper right')\n",
    "    \n",
    "\n",
    "    #accuracy plot\n",
    "    plt.subplot(1,2,2)\n",
    "    for histObj in histObjs:\n",
    "        plt.plot(histObj.history['acc'])\n",
    "        plt.plot(histObj.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(names, loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # min, max for loss and acc\n",
    "    for histObj in histObjs:\n",
    "        h=histObj.history\n",
    "        maxIdxTrain = np.argmax(h['acc'])\n",
    "        maxIdxVal  = np.argmax(h['val_acc'])\n",
    "        minIdxTrain = np.argmin(h['loss'])\n",
    "        minIdxVal  = np.argmin(h['val_loss'])\n",
    "        \n",
    "        strg='\\tTrain: Min loss {:6.3f} at {:3d} --- Max acc {:6.3f} at {:3d} | '+histObj.name\n",
    "        print(strg.format(h['loss'][minIdxTrain],minIdxTrain,h['acc'][maxIdxTrain],maxIdxTrain))\n",
    "        strg='\\tValidation : Min loss {:6.3f} at {:3d} --- Max acc {:6.3f} at {:3d} | '+histObj.name\n",
    "        print(strg.format(h['val_loss'][minIdxVal],minIdxVal,h['val_acc'][maxIdxVal],maxIdxVal))\n",
    "        print(len(strg)*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classification\n",
    "In the first tutorial notebook, we used the neural network (NN) as a regression tool. Classification is a second large area where deep neural networks are applied. \n",
    "Depending on the different classes we want to distinguish, we talk about\n",
    "\n",
    " * Binary classification\n",
    " * Multi-class classification\n",
    "\n",
    "We are still in the field of supervised learning, but the true values are now **discrete class labels**.\n",
    "\n",
    "In **binary classification** the target values are typically $y^{(i)} \\in \\{0,1\\}$,  like \"Cats=0\" and \"Dogs=1\", or Background and Signal. \n",
    "The dataset consists in this case of the multi-dimensional input vectors $\\mathbf{x}^{(i)}$, our data points, together with the true labels $y^{(i)} \\in \\{0,1\\}$.\n",
    "\n",
    "In order to make the connection between the discrete class label and the continuous NN output, we introduce a probabilistic interpretation of the NN. We assume that the NN represents some probability density $p(\\mathbf{x}|\\mathbf{w},\\mathbf{b})$, and we try to learn this density from the distribution of the labels in our data. \n",
    "\n",
    "A single node as the last layer with a sigmoid activation function, the **logistic function**, is a way to implement this. The logistic function  maps all input values into the range $[0,1]$, as it should be for a proper probability.\n",
    "\n",
    "<img src=\"img/Activation_logistic.png\" width=\"200\" >\n",
    "\n",
    "$$f(x)=\\frac{1}{1+\\exp({-x})}$$\n",
    "\n",
    "### Loss function: *Cross-entropy*\n",
    "\n",
    "A special loss function is needed for the training of such a network. \n",
    "The optimal parameters $\\mathbf{w,b}$ can be found by an Maximum Likelihood approach.\n",
    "The decision between 1/0 is a binomial process (Bernoulli trial), like tossing a coin. \n",
    "If we assume that the NN represents a probability estimate \n",
    "$\\widehat{y}=p(\\mathbf{x}|\\mathbf{w},\\mathbf{b})$, than the probability for being in class 1 will be just $\\widehat{y}$, and $1-\\widehat{y}$ for being in class 0. For a batch with $n_{batch}$ data points, we multiply the probability for each data point\n",
    "\n",
    "$$\\prod_\\textrm{k in class 1}^{n_{batch}} \\widehat{y}_k \n",
    "\\cdot \\prod_\\textrm{k in class 0}^{n_{batch}}(1-\\widehat{y}_k) \n",
    "= \\prod_k^{n_{batch}}  [ y_k \\widehat{y}_k + (1-y_k)(1-\\widehat{y}_k) ], \n",
    "$$\n",
    "\n",
    "and the -log likelihood becomes:\n",
    "$$-\\frac{1}{n_{batch}}\\sum_{i=1}^{n_{batch}}y_i\\log(\\widehat{y_i})+(1-y_i)\\log(1-\\widehat{y_i})$$ \n",
    "\n",
    "\n",
    " \n",
    "This expression is also known as **cross entropy**. The name is related to the fact that entropy is defined as the expectation value of log(probability): _Entropy_$\\,:=\\int log(p)p(x)dx$. If we take the average over some other probability $q(x)$, we talk about cross entropy, _Cross Entropy_$\\,:=\\int log(p)q(x)dx$. It can be shown that the cross entropy becomes minimal when the two probabilities are identical $p=q$. https://en.wikipedia.org/wiki/Gibbs%27_inequality<br>\n",
    "In addition, we have normalized by $1/n_{batch}$ that makes it easy to interpret,for example $\\frac{1}{n_{batch}}\\sum_{i=1}^{n_{batch}}y_i$ as probability for finding class 1 in the batch.\n",
    "\n",
    "In short, with cross entropy as loss function, the NN learns to model the probability distribution of the training labels $y^{(i)}$ in the data. The output of the NN, the prediction $\\widehat{y}$, is a number between [0,1], and by default, a threshold of 0.5 is applied to get a yes/no answer.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Example \n",
    "\n",
    "We create the same toy data as in part 1, but this time we only ask if the output is positive.\n",
    "\n",
    "$$y=\n",
    "\\begin{cases}\n",
    "    \\;1,& \\textrm{if }  \\sin(\\sum_{l=1}^{10} x_l^2) \\ge 0 \\\\\n",
    "    \\;0,& \\textrm{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We create 10000 random vectors each 10-dim\n",
    "N_samples=10000\n",
    "N_in=10\n",
    "# A matrix N_samplesxN_in, uniform in [0,1)\n",
    "x_train=np.random.rand(N_samples,N_in)\n",
    "# Sum of squares along N_in\n",
    "z = np.sum( np.square(x_train),axis=1)\n",
    "y_train = (np.sin(z) >= 0).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 20 target values. A sequence of 0 and 1\n",
    "print(y_train[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Validation sample\n",
    "\n",
    "We build a simple network to classify our toy data with 1 hidden layer and logistic function as output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "# To suppress deprecation warnings in some environments\n",
    "from tensorflow import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Dense(100, activation='relu', input_dim=10),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "optimizer = optimizers.Adam()\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "histObj = model.fit(x_train, y_train, batch_size=256, epochs=25, validation_split=0.2)\n",
    "#histObj = model.fit(x_train, y_train, batch_size=256, epochs=200, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    " There are a few things to note.\n",
    "* The shorthand name in Keras for the logistic function is just `sigmoid`.\n",
    "* Keras provides a cross entropy loss function for the binary case:<br> \n",
    "**binary_crossentropy** https://keras.io/backend/#sparse_categorical_crossentropy  https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy.\n",
    "* When we compile this model, we also add **accuracy** (1) as a **metric**: `metrics=['acc']`.<br>\n",
    "A metric is an additional quality measure, similar to the loss, but it is not used for the learning. \n",
    "* We define a validation sample for the fit method: `validation_split=0.2`. This means that 20% of our training data will be put aside and only be used for **validation** (2), i.e. for calculation the validation loss and validation accuracy. \n",
    "\n",
    "(1) Accuracy is calculated from the predictions of the trained network. It is defined as the ratio of the correctly classified labels over all data. \n",
    "\n",
    "(2) When we test the performance of a network on the same dataset as used for training, the result will be over-optimistic. The network typically learns some accidental patterns (green line below) from the datasets. The performance will not **generalize** to new, unseen data. To check for such **overfitting** or overtraining, \n",
    "<br>\n",
    "<img src=\"img/Overfitting_wikipedia.png\" width=\"200\" >\n",
    "https://en.wikipedia.org/wiki/Overfitting<br>\n",
    "it is mandatory to evaluate the loss and the metric(s) on an **independent validation sample** to control the learning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have now 2x2 histories, \n",
    "# for the 2 samples, training and validation,\n",
    "# and for the loss and the accuracy\n",
    "histObj.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values over epochs\n",
    "plt.plot(histObj.history['loss'])\n",
    "plt.plot(histObj.history['val_loss'])\n",
    "plt.title('model_output loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the loss drops quickly over the first epochs and then improves only slowly.\n",
    "Note the peculiarity of the Keras history object: In the first epochs, the validation loss is lower than the train loss. We would expect the opposite behavior: a better or at least similar performance on the train data due to overfitting. The reason is that the training loss is computed as the average of the loss per batch over all batches within an epoch. Because our model is changing with time, the loss over the first batches of an epoch is generally higher than over the last batches. In contrast, the validation loss is only calculated using the model at the end of each epoch, resulting in a lower loss. After some epochs, when the model does not change quickly anymore, and the overtraining becomes larger, we see the expected trend.\n",
    "\n",
    "The fact that train and validation loss do not diverge strongly shows that the chosen model is not too complex for our problem - maybe it is to simple. \n",
    "\n",
    "The loss is still falling after 20 epochs. A larger number of epochs will probably improve the training.   Try!\n",
    "\n",
    "The slowly improving part of the learning curve is the important part for learning subtle features of the data. The stochastic gradient decent learning gets into a more diffusion like phase where the weights only improve slowly, but this is also the phase where severe overtraining may appear. Typically, one stops the training when the loss do not improve within certain limits (**early stopping**).\n",
    "***\n",
    "The accuracy in the next plot follows mainly the loss improvement. It starts at about 0.5 - even a purely random label would be correct in 50% of the trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(histObj.history['acc'])\n",
    "plt.plot(histObj.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We can evaluate our model on a dataset: `model.evaluate` returns the metric(s) and loss(es) we have provided in the compile step. Note that a larger batch size also speeds up the calculation during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate our model on the complete training dataset \n",
    "# A larger batch size speeds up the calculations\n",
    "loss_value, accuracy = model.evaluate(x_train, y_train,batch_size=512)\n",
    "print(loss_value,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment, if you want to test the speed improvements yourself\n",
    "#%timeit y_pred=model.predict(x_train).reshape(y_train.shape) \n",
    "# 1 loop, best of 3: 189 ms per loop - Intel Core i7 2 core 2.5 GHz MacBook Pro\n",
    "#%timeit y_pred=model.predict(x_train,batch_size=512).reshape(y_train.shape)\n",
    "# 10 loops, best of 3: 17 ms per loop\n",
    "## The results maybe differ but only within the numerical precision\n",
    "#y_pred_a=model.predict(x_train).reshape(y_train.shape)\n",
    "#y_pred_b=model.predict(x_train,batch_size=512).reshape(y_train.shape)\n",
    "#print('numeric identical',np.sum(np.isclose(y_pred_a,y_pred_b)))\n",
    "y_pred=model.predict(x_train,batch_size=512).reshape(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy calculation by hand\n",
    "accuracy=np.mean(np.equal(y_train, y_pred>0.5))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Multi-Class Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often a problem consists in identifying multiple classes or categories. Each data point belongs to exactly one class, and the training data is labeled according to the $K$ classes: $y^{(i)}\\in\\{1,..,K\\}$. The classical example is the identification of images from handwritten numbers 0-9 (the MNIST dataset). https://deepai.org/machine-learning-glossary-and-terms/mnist-database\n",
    "\n",
    "The considerations presented above for binary classification can be applied in a similar way to multi-class classification.\n",
    "Instead of a binomial problem, we have now a multi-nomial problem, and the proper loss function becomes the **categorical cross entropy**.\n",
    "\n",
    "As in the binary case, we implement a probability interpretation of the NN output. The output layer must be extended to represent the $K$ classes. Instead of the one sigmoid, we must have $K$ nodes with output $z_k$, and we need an activation function that allows a probability interpretation. The appropriate activation function in this case is called **softmax**:\n",
    "\n",
    "$f_{k}(\\overrightarrow{z})=\\frac{\\exp({z_{k}})}{\\sum_{m=1}^{K} \\exp({z_{m}})} \\quad \\text { for } k=1, \\ldots, K$ https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "The softmax function is defined in a way that each output node stays within $[0,1]$, as it must be for a probability, and the sum of all outputs is always one ($\\sum_k f_k=1$) since by definition each data point must belong to one class. The predicted class is then the node with the largest softmax output.\n",
    "\n",
    "In short, for multi-classification we use $K$ nodes with softmax activation and (sparse, see below) categorical cross entropy as loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fashion-MNIST\n",
    "\n",
    "As multi-class example, we take the Fashion-MNIST dataset. This dataset had been become popular in last few years as a benchmark example in image classification. https://github.com/zalandoresearch/fashion-mnist\n",
    "(Zalando SE is a European e-commerce company based in Berlin selling fashion products.)\n",
    "\n",
    "Keras knows this dataset and can provides the images (or downloads them on first use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "dataset = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train),(x_val, y_val) = dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- by technical reasons we load the data locally during the labs\n",
    "#import gzip\n",
    "#import numpy as np\n",
    "#with gzip.open('./fashion-mnist/train-labels-idx1-ubyte.gz', 'rb') as bits:\n",
    "#    y_train = np.frombuffer(bits.read(), np.uint8, offset=8)\n",
    "#with gzip.open('./fashion-mnist/t10k-labels-idx1-ubyte.gz', 'rb') as bits:\n",
    "#    y_val  = np.frombuffer(bits.read(), np.uint8, offset=8)\n",
    "#with gzip.open('./fashion-mnist/train-images-idx3-ubyte.gz', 'rb') as bits:\n",
    "#    x_train = np.frombuffer(bits.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28)\n",
    "#with gzip.open('./fashion-mnist/t10k-images-idx3-ubyte.gz', 'rb') as bits:\n",
    "#    x_val = np.frombuffer(bits.read(), np.uint8, offset=16).reshape(len(y_val), 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 60k training images and 10k validation images. The images are grayscale (0-255 integer) with 28x28 pixels. There are 10 equal sized categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is always a good practice to normalize the dataset.\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "print(x_train.dtype) # float32 has an higher efficiency on GPUs - float64 with a CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first 20 labels from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technicality, Keras provides two versions of categorical cross entropy (CCE) loss. The choice depends on the way how the class labels are represented. There are essentially 2 possibilities: the one-hot-encoding, for example `[0,0,0,1,0,0,0,0,0,0]`, where the position marks the class, or just integer numbers, like '3'. From the previous cell, we see that the latter is used in the fashion-MNIST dataset. The appropriate implementation of the loss function in this case is the sparse CCE.\n",
    "***\n",
    "We define names for the classes, and plot a few training images to get an idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, fashion items in low res."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network\n",
    "\n",
    "A dense network with 2 hidden layers. The aim is to identify from the picture the class it belongs to.\n",
    "* As in the introduction described, we use a last layer with $K=10\\,$ nodes and the 'softmax' activation together with sparse categorical cross entropy.\n",
    "* The first layer flattens the images. The 28x28 image is transformed into a 784 vector. There are better ways to handle image data, e.g. (see below) convolution.\n",
    "* Accuracy as metric is included. There are now 10 categories and a randomly applied label would have a 10% chance to be correct. \n",
    "* Note that with a few lines of code we have defined a model with more than half a million of parameters.\n",
    "* For validation, we use now the separate validation dataset in the fit() method:`validation_data=(x_val,y_val)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "\n",
    "model1 = tf.keras.models.Sequential([\n",
    "  layers.Flatten(input_shape=(28, 28)),\n",
    "  layers.Dense(512, activation='relu'),\n",
    "  layers.Dense(512, activation='relu'),\n",
    "  layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "histObj1 = model1.fit(x_train, y_train, validation_data=(x_val,y_val), batch_size=256, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histObj1.name='' # name added to legend\n",
    "plotLearningCurves(histObj1) # the above defined function to plot learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the loss drops quickly over the first epochs and then improves only slowly. But this time, the **gap between validation and train** is significant, and the loss on the validation data does not much improve, whereas the train loss continues to fall. We create overtraining. The accuracy on the validation data is significantly smaller, the model does not generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate accuracy and loss on the trained model for the both samples\n",
    "loss_train, acc_train = model1.evaluate(x_train, y_train,batch_size=512)\n",
    "loss_val, acc_val   = model1.evaluate(x_val, y_val,batch_size=512)\n",
    "print(\"Training data loss %6.4f, acc %6.4f\" % (loss_train,acc_train))\n",
    "print(\"Validation data loss     %6.4f, acc %6.4f\" % (loss_val,acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human accuracy in this task is about 0.835. The most sophisticated algorithms push it up to 0.96.\n",
    "***\n",
    "Our predictions looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 5 first validation images\n",
    "import numpy as np\n",
    "sixImages = x_val[0:6]\n",
    "predictions = model1.predict(sixImages)\n",
    "predictions = np.argmax(predictions,axis=1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(6):\n",
    "    plt.subplot(1,6,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(sixImages[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel('pred.: '+class_names[predictions[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to prevent overtraining\n",
    "\n",
    "Overfitting is related to the complexity of the model in relation to the available data. We have 669706 parameters. A model with many parameters will be able to \"memorize\" the training data. Reducing the complexity of the model will improve the generalization ability. On the other hand, reducing the complexity of the model will reduce its predictive power. It is up to the model designer (you!) to find a good compromise, and there is no general applicable recipe for this task.\n",
    "\n",
    "You will have to experiment with the parameters of the network. The number of layers, the size of the layers, as well as the learning rate, the batch size etc. \n",
    "\n",
    "A possible strategy is to increase the complexity of the model until at least the training accuracy is on a sufficient level and then introduce **regularization** to the network. Regularization means that the loss is modified in a way to improve some aspect of the performance of a model.\n",
    "https://en.wikipedia.org/wiki/Regularization_(mathematics) For example, in \"weight regularization\" a cost related to the size of the weights is added.\n",
    "\n",
    "Keras provides several methods to make the network more robust.\n",
    "\n",
    "* Weight regularization https://keras.io/regularizers/\n",
    "* Batch normalization https://keras.io/layers/normalization/#batchnormalization\n",
    "* Dropout https://keras.io/layers/core/#dropout\n",
    "\n",
    "**Weight regularization** Even if we have sufficient data, the weights in a network may not be constraint. Consider two consecutive layers. The weights in the first layer may grow steadily and shrink in the second layer such that the total effect stays constant. Adding a penalty, e.g. the sum of the squares of the layer weights `l2`,to the loss helps to control the size of the weights .\n",
    "\n",
    "**Batch normalization** is similar to normalizing the input data but applied between the layers and for each batch. It normalizes the activations of the previous layer at each batch, i.e. transforms mean to 0 and standard deviation to 1.\n",
    "\n",
    "**Dropout** is the idea to remove nodes from a layer during training randomly at typical rate of $p=0.2-0.3$. From iteration to iteration suddenly a node may disappear. \n",
    "\n",
    "Play!\n",
    "\n",
    "* Reduce the 2 hidden layers to 256 and 128. The performance degrades (worse validation accuracy), but the gap between training and validation results becomes smaller (less overfitting).\n",
    "* Go back to the 2 hidden 512 layers and introduce weight regularization<br>\n",
    "(... `kernel_regularizer=regularizers.l2(0.0001)` ...)<br> \n",
    "to the first hidden layer. The train-validation gap should shrink, but probably also the best accuracy.\n",
    "* Go back to the 2 hidden 512 layers and include batch normalization (`layers.BatchNormalization()`) after the first hidden layer. Again, the train-validation gap should shrink, but probably also the best accuracy.\n",
    "* Go back to the 2 hidden 512 layers and include a dropout layer (`layers.Dropout(rate=0.2)`) after the first hidden layer. The gap between validation and train will become smaller but hopefully the performance stays good. Dropout regularizes the growth of the weights, too. If this performs reasonably, a few more epochs (50) may improve the model such that we touch the 90% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, losses, optimizers, regularizers\n",
    "\n",
    "model2 = tf.keras.models.Sequential([\n",
    "  layers.Flatten(input_shape=(28, 28)),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "#  layers.Dense(512, activation='relu',kernel_regularizer=regularizers.l2(0.0001)),\n",
    "#  layers.Dense(512, activation='relu'),\n",
    "#  layers.BatchNormalization(),\n",
    "#  layers.Dropout(rate=0.3),\n",
    "#-------\n",
    "layers.Dense(128, activation='relu'),\n",
    "#  layers.Dense(512, activation='relu'),\n",
    "  layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "histObj2 = model2.fit(x_train, y_train, validation_data=(x_val,y_val), batch_size=256, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histObj1.name=''\n",
    "histObj2.name='other' # put a useful name\n",
    "plotLearningCurves(histObj1,histObj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "\n",
    "* We missed the most important element for image classification: **convolutional neural networks** (CNNs). https://towardsdatascience.com/convolutional-neural-networks-for-beginners-practical-guide-with-python-and-keras-dc688ea90dca\n",
    "* Keras allows to define **callbacks** in the fit method to perform many useful tasks during the training, e.g. at the begin or end of each epoch. https://keras.io/callbacks/\n",
    "* We did not discuss the way the initial weights are initialized. https://keras.io/initializers/ \n",
    "* And many more things. https://keras.io/getting-started/sequential-model-guide/#examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
