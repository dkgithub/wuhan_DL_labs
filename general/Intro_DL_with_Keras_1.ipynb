{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with Keras 1\n",
    "\n",
    "### 5th Summer School on INtelligent signal processing for FrontIEr Research and Industry, Wuhan, May 2019\n",
    "\n",
    "This is a dense introduction to **deep learning** (DL)!\n",
    "\n",
    "For an in-depth introduction, there are a several free online books:\n",
    "\n",
    "* http://neuralnetworksanddeeplearning.com/index.html by Michael Nielsen, a concise introduction.\n",
    "* http://www.deeplearningbook.org/ Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville, provides a detailed introduction to the theoretical background.\n",
    "* https://torres.ai/first-contact-deep-learning-practical-introduction-keras/ by Jordi Torres, an introduction similar to this tutorial by the use of Keras.\n",
    "* https://d2l.ai/ by A. Zang et al., another excellent resource but with code examples in Gluon/MXNet (https://gluon.mxnet.io/) instead of Keras.\n",
    "* https://deepai.org/data-science-glossary/a A useful glossary!\n",
    "* https://developers.google.com/machine-learning/glossary/ another glossary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neural Network?\n",
    "### What is Machine Learning?\n",
    "In machine learning (ML) we try to adopt (or **learn**) a certain model to some data. \n",
    "\n",
    "Linear regression is the easiest example. Some data is given and a regression line (predicted value $\\widehat{y}$) is fitted to the data:\n",
    "\n",
    "$$\\widehat{y}(x)=ax+b$$\n",
    "\n",
    "with data $x^{(i)},y^{(i)} \\in R$ and $i=1 \\dots N_{data}\\;$ the data points in our dataset.\n",
    "\n",
    "There are two principal ways to do machine learning:\n",
    "\n",
    "* Supervised\n",
    "* Unsupervised\n",
    "\n",
    "We are interested in **supervised machine learning**. Supervised means that we have some data where we know the true values, for example the $y$-values in the regression problem where we know for each $x_i$ the corresponding $y_i$. \n",
    "\n",
    "The 2 parameters ($a$,$b$) of the fit (regression line) are then extracted from the dataset $\\{x^{(i)},y^{(i)}\\}$, for example by a Least Square approach where we choose $a,\\;b$ such that the squares are minimized:\n",
    "\n",
    "$$\\min_{wrt. a,b} \\sum_{i=1}^{N_{data}} (\\widehat{y}\\,(x^{(i)}|\\,a,b)-y^{(i)})^2$$\n",
    "\n",
    "This sum of squares is an example for a **loss function**: The complete dataset is mapped onto one number which represents the quality how well our model describes the data. The best model parameters are given by the set of parameters ($a$,$b$) that **minimizes** the loss. \n",
    "Other names for loss are objective or cost function.\n",
    "\n",
    "While this simple regression model only contains 2 parameters, a typical deep neural network may contain ~1000000 or more parameters, and our dataset will be\n",
    "in general high dimensional $\\mathbf{x}^{(i)}=(x^{(i)}_1,\\dots,x^{(i)}_n) \\in \\cal{R}^n$. BTW the input components $x_i$ are traditionally called **features** in machine learning.\n",
    "\n",
    "Supervised learning can be subdivided into:\n",
    "* Regression\n",
    "* Classification\n",
    "\n",
    "In **classification**, we do not have a real value $y$ but a class label $y^{(i)}=\\{0,1\\}$. For example, when the dataset consists of two groups, like \"Cats=0\" and \"Dogs=1\" or Background and Signal. First, the regression case is considered. For classification see the \"Intro_DL_with_Keras_2\" notebook.\n",
    "\n",
    "\n",
    "### Build a neural network (NN)\n",
    "\n",
    "* A neural network consists of several **layers**.\n",
    "* Each layer consists of several **nodes**. The nodes are also called **neurons**.\n",
    "\n",
    "The node is considered to be a model for a neuron. Like a neuron, it has several inputs $x_k$  and one output. Each input is multiplied with a **weight** $w_k$ and the output depends on the sum of the inputs (and a **bias** term $b$)  transformed by some **activation function** $\\mathbf{f}$.\n",
    "\n",
    "$$f(\\sum_{k=1}^n w_k x_k +b)$$\n",
    "\n",
    "<img src=\"img/sum.png\" width=\"200\" >\n",
    "\n",
    "When several neurons are joined into a Layer, we get multiple outputs that we can represent by a vector and the weights become a weight matrix (dropping the bias $b$ for the simplicity of the image):\n",
    "\n",
    "<img src=\"img/nodes.png\" width=\"200\" >\n",
    "\n",
    "That we talk about neurons is a mainly a historical artefact. We have a multi-linear model, **each layer is a matrix multiplication**, where each component is then mapped by a non-linear activation function.\n",
    "\n",
    "$$f[\\mathbf{W}\\cdot \\overrightarrow{x} +\\overrightarrow{b}]$$\n",
    "\n",
    "Here, we write $f[x]$ to note that the activation function is applied to each component separately. \n",
    "\n",
    "\n",
    "Complex models are built by chaining multiple layers with appropriate activation functions.\n",
    "\n",
    "\n",
    "<img src=\"img/net.png\" width=\"300\" > \n",
    "\n",
    "For more complicated networks, the layers will become multi-dimensional arrays not just matrices. The most general way to describe the node is to consider it as a tensor (that's why it is called Tensorflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "A chain of matrix operation would be trivial. \n",
    "All non-linearity of a NN comes from the activation functions.\n",
    "There are several choices for the function that is wrapped around the linear node:\n",
    "$f[\\mathbf{W}\\cdot \\overrightarrow{x} +\\overrightarrow{b}]$. https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "***\n",
    "\n",
    "Early (1990-2000) neural networks had mainly been constructed with the hyperbolic tangent:\n",
    "\n",
    "<img src=\"img/Activation_tanh.png\" width=\"200\" >\n",
    "$$\\tanh$$\n",
    "\n",
    "***\n",
    "The most important activation function is now the _Rectified Linear Unit_, short **ReLU**. Introducing the ReLU had been a game changer for the training of deep neural networks. \n",
    "\n",
    "<img src=\"img/Activation_rectified_linear.png\" width=\"200\" > \n",
    "$$f(x)=\\left\\{\\begin{array}{ll}{0} & {\\text { for } x<0} \\\\ {x} & {\\text { for } x \\geq 0}\\end{array}\\right.$$\n",
    "\n",
    "***\n",
    "A third important activation is the **logistic function**, aka sigmoid. \n",
    "The logistic function is the optimal choice if the NN represents a probability. It maps all input into the range $[0,1]$, as it should be for a proper probability.\n",
    "\n",
    "<img src=\"img/Activation_logistic.png\" width=\"200\" >\n",
    "$$f(x)=\\frac{1}{1+\\exp({-x})}$$\n",
    "\n",
    "***\n",
    "\n",
    "## TensorFlow & Keras\n",
    "\n",
    "Neural nets are built with the help of deep learning libraries, for example **Tensorflow** https://www.tensorflow.org (in China http://www.tensorfly.cn). \n",
    "Since the Tensorflow \"language\" is somewhat complicated there are additional libraries that simplify\n",
    "the building of neural networks. They add an additional layer on top of the actual deep learning library. A popular choice for such a wrapper is **Keras**. \n",
    "<br>\n",
    "Keras had been originally developed as an independent library (which still exists https://keras.io/) but it has been meanwhile integrated into Tensorflow https://www.tensorflow.org/api_docs/python/tf/keras and has become the default approach in the coming Tensorflow 2.0.\n",
    "\n",
    "Tensorflow or Keras are Python libraries that provide all tools to define a neural network and to train them.\n",
    "<br>\n",
    "The style in which the matrix operations are implemented **resembles NumPy**. The data is represented by multi-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the keras implementation from tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,models\n",
    "# To suppress deprecation warnings in some environments\n",
    "from tensorflow import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network with Keras\n",
    "\n",
    "A layer is the base element to build a NN. In Keras, the simplest layer is the `Dense` class.\n",
    "<br>\n",
    "**`layers.Dense(units,activation)`**\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "<br>\n",
    "The minimum information Keras needs are the number _units_ $\\equiv$ _nodes_ $\\equiv$ _neurons_ within this layer and the activation function. \n",
    "<br>\n",
    "The following creates a layer with 30 nodes where each node uses the **ReLU** function as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer = layers.Dense(30, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A NN is a chain of layers. We need a way to put several layers into one model. \n",
    "<br>\n",
    "In Keras all networks belongs to the `model` class and the simplest `model` is just a sequential chain of layers\n",
    "<br>\n",
    "**models.Sequential( list_of_layers )** https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(20, activation='relu', input_dim=10),\n",
    "    layers.Dense(30, activation='relu')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, a layer is (mainly) a matrix, as such it has two dimensions, the number of inputs and the number of outputs. Keras tries to figure out the correct array dimensions from the previous layer. For the first layer we have to give the number of inputs, **input_dim**, explicitly.\n",
    "\n",
    "If we write this model down as a formula:\n",
    "\n",
    "$$\\text{model} = \\mathrm{ReLu}\\Big[ W_2 \\cdot\\mathrm{ReLu}\\big[W_1\\cdot x+b_1\\big] + b_2\\Big]$$\n",
    "\n",
    "Keras figured out that the first layer is a 10x20 array (matrix) and the second layer a 20x30 array (to match the dimension of the previous layer). By default, each node gets a bias term. Our model has 20+30=50 bias terms. It contains in total 850 parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary() prints the list of layers and parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see\n",
    "* The type of the layers and order in the model\n",
    "* The output shape of each layer (Here, the 2nd dim is None.)\n",
    "* The number of parameters (weights, biases) in each layer\n",
    "\n",
    "The layers between the input and output layer are called **hidden layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can add a layer to an already defined model\n",
    "model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can plot the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)\n",
    "from IPython.display import Image\n",
    "Image(retina=True, filename='model.png')\n",
    "# The box with large integer in the first box is a bug in plot_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Loss function\n",
    "\n",
    "Such a model can be used for regression. As described above, we are doing supervised learning. That means we know some target value $y$, and we can quantify the difference between our model and the true value with the help of some loss function.\n",
    "\n",
    "Keras provides the mean squared error loss function, we mentioned above.<br>\n",
    "**losses.mean_squared_error(y_true, y_pred)** https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError\n",
    "\n",
    "### Example\n",
    "\n",
    "Before we continue, we need some example data to play with. We will create some random toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# We create 10000 random vectors each 10-dim\n",
    "N_samples=10000\n",
    "N_in=10\n",
    "# A matrix N_samplesxN_in, uniform in [0,1)\n",
    "x_train=np.random.rand(N_samples,N_in)\n",
    "# Sum of squares along N_in\n",
    "z = np.sum( np.square(x_train),axis=1)\n",
    "y_train = np.sin(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The toy data consists of a sine over a parabola in a 10-dimensional space:\n",
    "\n",
    " $$y=\\sin(\\sum_{k=1}^{10} x_k^2)$$\n",
    "\n",
    "The positions $\\mathbf{x}$ are randomly picked from $x_k\\in\\left[0,1\\right]$. If we plot this, there is no easy visible connection between the $\\{x_k\\}\\;$ and $y$. This is a good example for some data that looks random but  contains some hidden structure that can be learned by a NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(1,6,figsize=(10,2))\n",
    "fig.tight_layout()\n",
    "axs[0].set_ylabel(r'$y=sin(\\vec{x}^2$)')\n",
    "for i in range(5):\n",
    "    idx=np.argsort(x_train[:200,i]) # we plot only the first 200 samples\n",
    "    axs[i].plot(x_train[idx,i],y_train[idx])\n",
    "    axs[i].set_xlabel(r'$x_{}$'.format(i))\n",
    "z=np.sum(np.square(x_train[:200]),axis=1)\n",
    "idx=np.argsort(z)\n",
    "plt.plot(z[idx],y_train[idx])\n",
    "plt.show()\n",
    "# As example, the first 5 inputs - they just look like noise\n",
    "# but the information (sine curve) is hidden in the data.\n",
    "# We can see it when we calculate the square and sort the values \n",
    "#(argsort gives an index such that the array values are accessed in increasing order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Network optimiser & compilation\n",
    "\n",
    "Back to our neural network. What we have done so far is summarized in the next cell. We have built a sequential model with 3 layers and ReLU activation functions, and we choose the **Mean Square Error** as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "from time import time\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Dense(20, activation='relu', input_dim=10),\n",
    "        layers.Dense(30, activation='relu'),\n",
    "#        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")\n",
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "#optimizer = optimizers.SGD(lr=0.02,momentum=0.95)\n",
    "#optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer,loss='mse')\n",
    "\n",
    "#start_time=time()\n",
    "#histObj = model.fit(x_train, y_train, batch_size=32, epochs=20)\n",
    "#print('Total training time:{:6.2f} sec'.format(time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last line in the above cell, we configure the complete Keras model for training. This is done by the `compile` method.\n",
    "\n",
    "There are two new elements to note:\n",
    "\n",
    "* The Loss function which is simply giving by the named argument:`loss=`'mse'.\n",
    "\n",
    "* The **optimizer** which is defined by providing an instance as argument.\n",
    "\n",
    "Up to now we only said that the parameters should be _optimized_ in a way that the loss becomes minimal. For the NN training this is done technically by some variant of **Stochastic Gradient Descent** https://en.wikipedia.org/wiki/Stochastic_gradient_descent.\n",
    "\n",
    "Gradient descent is an optimization method where we calculate the gradient of our loss function with respect to the NN parameters (weights and biases). This gradient is iteratively used to update the parameter estimates until we reach, step by step, the best parameter values $\\;\\equiv\\;$ the minimum of the loss function.\n",
    "\n",
    "_Stochastic_ gradient descent (SGD) is a variant where we use only a random subsample of our data, the **Batch**, for one iteration step and not the full dataset. This makes the process noisier and helps to avoid local minima. Batching the data points is also computationally useful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Training\n",
    "\n",
    "Now, we can train our model!\n",
    "\n",
    "* The data is grouped in **batch**es of 32 data points.\n",
    "    * The data points in the batches are picked in random order until all data points from the dataset are used.\n",
    "* One **Epoch** is over when all data points have been used once.\n",
    "    * The training continue with the same data according to the chosen number of epochs.\n",
    "\n",
    "We start with a batch size of 32. This is traditionally considered to be a good compromise between performance and speed.\n",
    "* Try to use a larger batch size. The training should be faster, but the loss may stay higher. The number of iterations per epoch will be smaller. The total number of iterations at the end of the training are important.\n",
    "* We can also change the **learning rate** `lr`. A too small lr will result in slow learning. With a too high lr, the training may not converge.\n",
    "\n",
    "Observe the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you re-run this cell within the jupyter notebook, the model will not be reset\n",
    "# Rerun the above cell with the model definitions \n",
    "# to make sure that your model is freshly initialized\n",
    "histObj = model.fit(x_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve\n",
    "The change of the loss over the training steps is called **learning curve**. Keras returns the `history` class when we do the `model.fit` step.\n",
    "The history is recorded by default for each epoch in the `history` dictionary. We can add different metrics here. By default, it contains the loss after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(histObj.history.keys())\n",
    "histObj.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss development over the epochs\n",
    "plt.plot(histObj.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning is the most important tool to control the learning process! The typical behavior behaviour is a quick drop in the beginning and slow improvement over the following epochs. As long as the curve is still falling it may be useful to continue the learning by choosing a larger number of epochs, although it often improves at some point only asymptotically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting\n",
    "Predicting with the trained network is easy. Just apply it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we plot the truth\n",
    "z=np.sum(np.square(x_train),axis=1)\n",
    "idx=np.argsort(z)\n",
    "plt.plot(z[idx],y_train[idx])\n",
    "plt.ylabel(r'$y_{true}$')\n",
    "plt.xlabel(r'$|x|^2$')\n",
    "plt.show()\n",
    "\n",
    "# Next, we plot the NN output\n",
    "plt.plot(z[idx],y_pred[idx])\n",
    "plt.ylabel(r'$y_{predicted}$')\n",
    "plt.xlabel(r'$|x|^2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last plot shows the NN prediction. It probably will not yet look like a sine. There are several ways to improve the model.\n",
    "\n",
    "* Add an additional layer to the network, increase the number of nodes in the first 2 layers.\n",
    "* Add momentum to the SGD optimizer (8.3.6) in https://www.deeplearningbook.org/contents/optimization.html.\n",
    "* Or use a more refined optimizer. **Adam** is the _swiss army knife_ in NN training. It tries to adapt learning rate and other parameters automatically during the training.\n",
    "* Increase the number of epochs, maybe 50.\n",
    "* We aim for a loss of about $10^{-4}$ but be aware that the sine will stay noisy. \n",
    "* E.g.: (batch_size=256, adam(lr=0.001), 3 layers with 100 nodes) \n",
    "* There is no unique way to find the optimal training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "\n",
    "* A neural network can be seen as a **function approximator**. Similar to a Taylor expansion or a Fourier Series, one can prove that a NN with at least one hidden layer can approximate any function https://en.wikipedia.org/wiki/Universal_approximation_theorem, http://neuralnetworksanddeeplearning.com/chap4.html.\n",
    "* The optimizers used to train the networks have been developed over decades. They have reached a high level of sophistication and they are still subject to active research https://en.wikipedia.org/wiki/Stochastic_gradient_descent. There are many variants which are often picked for best performance in a trial and error approach.\n",
    "* At the heart of all deep learning libaries are automatic differentiation tools. For minimizing with respect to all trainable weights and biases, the derivative of the loss function and the complete(!) neural network is needed. This is called **backpropagation**, the difference between the true and predicted values are _back-propagated_ in linear approximation from the loss to the trainable parameters http://neuralnetworksanddeeplearning.com/chap2.html.\n",
    "The derivatives needed for this are calculated automatically from the code you define https://www.tensorflow.org/tutorials/eager/automatic_differentiation. \n",
    "* How we connect the individual nodes, i.e. the topology of the network, allows for a huge variaty of different models. We have seen here only a simple, **fully connected $\\;\\mathbf{\\equiv}\\;$ dense neural network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
